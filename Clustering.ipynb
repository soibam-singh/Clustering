{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0714bdaa",
   "metadata": {},
   "source": [
    "1. What is unsupervised learning in the context of machine learning?\n",
    "- Unsupervised learning is a type of machine learning where the algorithm is given data without labeled responses‚Äîmeaning it doesn‚Äôt know the \"right answers\" ahead of time. Instead of learning from examples with known outcomes (like in supervised learning), the algorithm tries to identify patterns, groupings, or structure in the data on its own.\n",
    "2. How does K-Means clustering algorithm work? \n",
    "- **K-Means** is an **unsupervised machine learning algorithm** that groups data into **K distinct clusters** based on similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Step-by-Step Process:\n",
    "\n",
    "1. **Choose the number of clusters (K)**  \n",
    "   Decide how many groups (clusters) you want to divide your data into.\n",
    "\n",
    "2. **Initialize centroids**  \n",
    "   Randomly select **K data points** as the initial cluster centers (called **centroids**).\n",
    "\n",
    "3. **Assign each data point to the nearest centroid**  \n",
    "   For every point in the dataset:\n",
    "   - Calculate its distance to each centroid.\n",
    "   - Assign it to the closest one (usually using **Euclidean distance**).\n",
    "\n",
    "4. **Update centroids**  \n",
    "   Recalculate the centroids by taking the **mean** of all data points assigned to each cluster.\n",
    "\n",
    "5. **Repeat** steps 3 and 4  \n",
    "   - Continue assigning points and updating centroids until:\n",
    "     - The centroids don‚Äôt move much (convergence), or\n",
    "     - A maximum number of iterations is reached.\n",
    "\n",
    "---\n",
    "3. Explain the concept of a dendrogram in hierarchical clustering.\n",
    "-\n",
    "A **dendrogram** is a tree-like diagram that records the sequences of merges or splits in **hierarchical clustering**. It helps visualize how clusters are formed and the relationships between data points.\n",
    "\n",
    "---\n",
    "\n",
    "### üå≥ What Does a Dendrogram Represent?\n",
    "\n",
    "- Each **leaf node** at the bottom represents a single data point.\n",
    "- As you move **up the tree**, data points and clusters are merged together.\n",
    "- The **height** at which two clusters are merged represents the **distance** or **dissimilarity** between them.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç How to Interpret a Dendrogram:\n",
    "\n",
    "1. **Short vertical lines**: Clusters that are similar (low dissimilarity).\n",
    "2. **Tall vertical lines**: Clusters that are different (high dissimilarity).\n",
    "3. **Cutting the dendrogram**:\n",
    "   - You can select a level (a horizontal line) to \"cut\" the tree and form a specific number of clusters.\n",
    "   - The number of vertical lines intersected by the cut determines the number of clusters.\n",
    "\n",
    "---\n",
    "\n",
    "### üß¨ Types of Hierarchical Clustering:\n",
    "\n",
    "- **Agglomerative (bottom-up)**:\n",
    "  - Start with each data point as its own cluster.\n",
    "  - Iteratively merge the closest clusters.\n",
    "- **Divisive (top-down)**:\n",
    "  - Start with all data points in one cluster.\n",
    "  - Recursively split clusters into smaller ones.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Why Use a Dendrogram?\n",
    "\n",
    "- Helps understand the **hierarchical relationships** between data points.\n",
    "- Useful for choosing the **optimal number of clusters** by visually inspecting where large jumps in height occur.\n",
    "- Can be used with various **linkage methods** (e.g., single, complete, average) to define cluster distance.\n",
    "\n",
    "---\n",
    "4. What is the main difference between K-Means and Hierarchical Clustering?\n",
    "-  ### üìä Summary Table\n",
    "\n",
    "| Feature                  | K-Means                  | Hierarchical Clustering     |\n",
    "|--------------------------|--------------------------|------------------------------|\n",
    "| Type                     | Partitioning             | Hierarchical (Agglomerative/Divisive) |\n",
    "| Requires K?              | Yes                      | No                           |\n",
    "| Reassign points?         | Yes                      | No                           |\n",
    "| Output                   | Flat clusters            | Dendrogram (tree)            |\n",
    "| Scalability              | High (fast)              | Low (slow on large data)     |\n",
    "| Cluster Shape            | Assumes spherical        | Can capture complex shapes   |\n",
    "\n",
    "---\n",
    "5. What are the advantages of DBSCAN over K-Means?\n",
    "- \n",
    "## üìä Summary Table\n",
    "\n",
    "| Feature                      | DBSCAN                         | K-Means                       |\n",
    "|------------------------------|--------------------------------|-------------------------------|\n",
    "| Requires Number of Clusters? | ‚ùå No                          | ‚úÖ Yes                        |\n",
    "| Handles Noise/Outliers?      | ‚úÖ Yes                         | ‚ùå No                         |\n",
    "| Cluster Shape Flexibility    | ‚úÖ Arbitrary shapes            | ‚ùå Spherical only             |\n",
    "| Cluster Size/Density         | ‚úÖ Varies                      | ‚ùå Assumes similar sizes      |\n",
    "| Algorithm Type               | Density-based                  | Partitioning-based            |\n",
    "\n",
    "---\n",
    "6. When would you use Silhouette Score in clustering?\n",
    "- ## üß† When to Use the Silhouette Score in Clustering\n",
    "\n",
    "The **Silhouette Score** is a useful metric for evaluating the quality of clusters in clustering algorithms, particularly when you're uncertain about the optimal number of clusters or how well your clusters are formed.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **To Evaluate the Quality of Clusters**\n",
    "- **Silhouette Score** helps assess how well-separated and cohesive your clusters are. A higher score indicates better clustering, where:\n",
    "  - **Positive score (close to +1)**: Points are well clustered and far from neighboring clusters.\n",
    "  - **Score around 0**: Points are on or near the decision boundary between clusters.\n",
    "  - **Negative score**: Points might be misclassified or in the wrong cluster.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **To Choose the Optimal Number of Clusters (K)**\n",
    "- Especially useful with algorithms like **K-Means**, where you need to determine the **best value of K**.\n",
    "- Calculate the Silhouette Score for different values of K (e.g., K = 2, 3, 4,...) and choose the one with the highest average score.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **To Compare Different Clustering Algorithms**\n",
    "- Use the Silhouette Score to **compare clustering performance** across different algorithms (e.g., K-Means vs DBSCAN vs Hierarchical).\n",
    "- A higher score generally indicates better performance on the given dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **When You Have Uncertainty About Cluster Quality**\n",
    "- If you're unsure whether the clusters are meaningful:\n",
    "  - The score gives a numerical evaluation of cluster strength.\n",
    "  - Particularly helpful with noisy or overlapping data.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **To Diagnose Misclassifications**\n",
    "- Can identify points that are **misclassified** or on **cluster boundaries**:\n",
    "  - A low or negative Silhouette Score suggests that the point may be poorly assigned.\n",
    "\n",
    "---\n",
    "7. What are the limitations of Hierarchical Clustering?\n",
    "- \n",
    "\n",
    "Hierarchical Clustering is a popular clustering technique, but it has several limitations:\n",
    "\n",
    "## 1. Computational Complexity\n",
    "- **Time Complexity**:\n",
    "  - **Agglomerative** (bottom-up) approaches typically have \\(O(n^3)\\) time complexity (or \\(O(n^2 \\log n)\\) with optimizations).\n",
    "  - **Divisive** (top-down) approaches are even worse, often \\(O(2^n)\\).\n",
    "- **Space Complexity**: Requires \\(O(n^2)\\) memory to store the distance matrix, making it inefficient for large datasets.\n",
    "\n",
    "## 2. Sensitivity to Noise and Outliers\n",
    "- Outliers can distort the hierarchy, leading to poor clustering results.\n",
    "\n",
    "## 3. Irreversibility of Merges/Splits\n",
    "- Once clusters are merged (agglomerative) or split (divisive), the decision cannot be undone, even if it later leads to suboptimal clusters.\n",
    "\n",
    "## 4. Difficulty in Choosing the Right Number of Clusters\n",
    "- Unlike K-means, hierarchical clustering does not suggest an optimal number of clusters. The dendrogram must be manually analyzed, and the cut-off point is subjective.\n",
    "\n",
    "## 5. Sensitivity to Distance Metric and Linkage Criteria\n",
    "- Different linkage methods (**single**, **complete**, **average**, **Ward‚Äôs**) can produce vastly different results.\n",
    "- The choice of **distance metric** (Euclidean, Manhattan, cosine) also heavily influences clustering.\n",
    "\n",
    "## 6. Not Scalable to Large Datasets\n",
    "- Due to high computational and memory requirements, it is impractical for datasets with millions of points.\n",
    "\n",
    "## 7. Assumption of Hierarchical Structure\n",
    "- Not all datasets have a natural hierarchy, making the method less effective for flat cluster structures.\n",
    "\n",
    "## 8. Difficulty in Handling Different Cluster Densities\n",
    "- Struggles when clusters have varying densities or non-spherical shapes.\n",
    "\n",
    "## When to Use Hierarchical Clustering?\n",
    "- **Small to medium-sized datasets** (a few thousand points).\n",
    "- When the data has a **hierarchical structure** (e.g., biological taxonomies).\n",
    "- When **interpretability via dendrograms** is important.\n",
    "\n",
    "8. Why is feature scaling important in clustering algorithms like K-Means?\n",
    "- \n",
    "Feature scaling is a critical preprocessing step for K-Means and other distance-based clustering algorithms. It ensures fair contribution from all features in the clustering process.\n",
    "\n",
    "### 1. Distance-Based Algorithm Sensitivity\n",
    "- K-Means uses **Euclidean distance** to measure similarity between points\n",
    "- Features with larger scales dominate distance calculations\n",
    "- Example:\n",
    "  - Age (20-40) vs. Salary (50,000-100,000)\n",
    "  - Salary would have 1000x more influence without scaling\n",
    "\n",
    "### 2. Equal Feature Contribution\n",
    "- Brings all features to comparable ranges:\n",
    "  - Standardization: mean=0, std=1\n",
    "  - Normalization: range=[0,1]\n",
    "- Prevents bias toward high-magnitude features\n",
    "\n",
    "### 3. Improved Algorithm Performance\n",
    "- Faster convergence during centroid updates\n",
    "- More stable cluster formation\n",
    "- Reduced risk of getting stuck in local optima\n",
    "\n",
    "### 4. Better Cluster Quality\n",
    "- More meaningful cluster separation\n",
    "- Easier interpretation of results\n",
    "- Reduced distortion from scale differences\n",
    "\n",
    "## Recommended Scaling Methods\n",
    "\n",
    "| Method | Formula | Best For |\n",
    "|--------|---------|----------|\n",
    "| **Standardization** (Z-score) | `(X - Œº)/œÉ` | Gaussian distributions |\n",
    "| **Min-Max Scaling** | `(X - min)/(max - min)` | Bounded ranges |\n",
    "| **Robust Scaling** | `(X - median)/IQR` | Data with outliers |\n",
    "\n",
    "9. How does DBSCAN identify noise points?\n",
    "-\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) identifies **noise points** as those that do **not belong to any cluster**. Here's how it works:\n",
    "\n",
    "### 1. Core Point\n",
    "A point is a **core point** if it has at least `minPts` points (including itself) within a distance `Œµ` (epsilon).\n",
    "\n",
    "### 2. Border Point\n",
    "A **border point** is within `Œµ` of a core point but does not have enough neighbors to be a core point itself.\n",
    "\n",
    "### 3. Noise Point\n",
    "A **noise point** is:\n",
    "- **Not a core point**, and\n",
    "- **Not within `Œµ` of any core point**\n",
    "\n",
    "11. What is the elbow method in K-Means clustering?\n",
    "-\n",
    "The **Elbow Method** is a technique used to determine the **optimal number of clusters (k)** in K-Means clustering.\n",
    "\n",
    "## How it Works:\n",
    "1. Run K-Means clustering on the dataset for a range of values of `k` (e.g., from 1 to 10).\n",
    "2. For each `k`, compute the **Within-Cluster Sum of Squares (WCSS)** ‚Äî also known as **inertia**.\n",
    "3. Plot the WCSS against the number of clusters `k`.\n",
    "4. The resulting graph looks like an **arm**. The \"elbow\" point is where the WCSS begins to decrease more slowly.\n",
    "\n",
    "## Interpretation:\n",
    "- The **\"elbow\" point** indicates the value of `k` where increasing the number of clusters yields **diminishing returns**.\n",
    "- This is considered the most **optimal number of clusters**, balancing accuracy and complexity.\n",
    "\n",
    "---\n",
    "12. Describe the concept of \"density\" in DBSCAN.\n",
    "-\n",
    "In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), **density** refers to how closely packed the data points are in a region.\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "- **Œµ (epsilon)**: The radius around a point used to define its neighborhood.\n",
    "- **minPts**: The minimum number of points required in an Œµ-neighborhood to consider a point a **core point**.\n",
    "\n",
    "## Types of Points Based on Density:\n",
    "\n",
    "1. **Core Point**: A point with **at least `minPts`** within its Œµ-radius. This indicates a **dense region**.\n",
    "2. **Border Point**: A point that is **within Œµ of a core point** but has **fewer than `minPts`** in its own Œµ-radius.\n",
    "3. **Noise Point (Outlier)**: A point that is **not a core point** and **not close enough to any core point** ‚Äî i.e., it's in a **low-density area**.\n",
    "\n",
    "## Summary:\n",
    "Density in DBSCAN is defined by the number of data points within a specified distance (`Œµ`). Clusters are formed in regions of **high point density**, and areas with **low point density** are marked as **noise** or **outliers**.\n",
    "\n",
    "---\n",
    "13. Can hierarchical clustering be used on categorical data?\n",
    "- # 13. Can Hierarchical Clustering Be Used on Categorical Data?\n",
    "\n",
    "Yes, **hierarchical clustering** can be used on **categorical data**, but it requires careful choice of distance (or similarity) metrics.\n",
    "\n",
    "## Key Points:\n",
    "\n",
    "- Traditional hierarchical clustering relies on **distance metrics** like **Euclidean distance**, which are not suitable for categorical variables.\n",
    "- For categorical data, you need to use **alternative similarity measures**, such as:\n",
    "  - **Hamming Distance**: Counts the number of mismatches between two categorical vectors.\n",
    "  - **Jaccard Similarity**: Measures similarity between sets (useful for binary/categorical features).\n",
    "  - **Simple Matching Coefficient (SMC)**: Ratio of matches to total attributes.\n",
    "\n",
    "## Steps:\n",
    "1. Compute a **distance matrix** using a suitable metric for categorical data.\n",
    "2. Apply hierarchical clustering (e.g., **agglomerative** or **divisive**).\n",
    "3. Visualize with a **dendrogram** to decide the number of clusters.\n",
    "\n",
    "---\n",
    "14. What does a negative Silhouette Score indicate?\n",
    "- # 14. What Does a Negative Silhouette Score Indicate?\n",
    "\n",
    "A **negative Silhouette Score** indicates that a data point is **likely assigned to the wrong cluster**.\n",
    "\n",
    "## Silhouette Score Overview:\n",
    "- Measures how similar a point is to its **own cluster** (cohesion) compared to **other clusters** (separation).\n",
    "- Range: **-1 to +1**\n",
    "  - **+1**: Well clustered, far from neighboring clusters.\n",
    "  - **0**: On or very close to the decision boundary between clusters.\n",
    "  - **-1**: Possibly assigned to the **wrong cluster**.\n",
    "\n",
    "## Interpretation of Negative Scores:\n",
    "- The average distance to other points in the **same cluster** is **greater** than the distance to points in a **different cluster**.\n",
    "- Indicates **poor clustering structure** or that the clusters are **overlapping or not well separated**.\n",
    "\n",
    "---\n",
    "15. Explain the term \"linkage criteria\" in hierarchical clustering.\n",
    "-\n",
    "**Linkage criteria** determine how the **distance between clusters** is calculated during the hierarchical clustering process (especially in **agglomerative clustering**).\n",
    "\n",
    "## Purpose:\n",
    "When two clusters are considered for merging, the linkage criteria decide **how the distance between them is computed** based on the distances between their individual points.\n",
    "\n",
    "## Common Linkage Methods:\n",
    "\n",
    "1. **Single Linkage** (Minimum Linkage):\n",
    "   - Distance between the **closest pair** of points from each cluster.\n",
    "   - Can result in **\"chaining\"** ‚Äî long, thin clusters.\n",
    "\n",
    "2. **Complete Linkage** (Maximum Linkage):\n",
    "   - Distance between the **farthest pair** of points.\n",
    "   - Tends to create **compact, spherical clusters**.\n",
    "\n",
    "3. **Average Linkage**:\n",
    "   - **Average distance** between all pairs of points across two clusters.\n",
    "   - A balance between single and complete linkage.\n",
    "\n",
    "4. **Ward‚Äôs Method**:\n",
    "   - Minimizes the **total within-cluster variance**.\n",
    "   - Tends to produce clusters of **similar size**.\n",
    "\n",
    "## Summary:\n",
    "Linkage criteria define **how clusters are merged** based on the distance between them. The choice of linkage affects the shape and size of resulting clusters.\n",
    "\n",
    "---\n",
    "16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?\n",
    "- K-Means assumes that clusters are **spherical**, **equally sized**, and have **similar densities**. When this assumption doesn't hold, performance degrades.\n",
    "\n",
    "## Reasons for Poor Performance:\n",
    "\n",
    "1. **Varying Cluster Sizes**:\n",
    "   - K-Means assigns points to the **nearest cluster centroid**.\n",
    "   - Larger clusters may be **split**, and smaller clusters may be **merged** into others.\n",
    "   - Leads to **incorrect assignments**.\n",
    "\n",
    "2. **Varying Densities**:\n",
    "   - K-Means uses **Euclidean distance**, which doesn‚Äôt account for differences in **density**.\n",
    "   - Dense clusters may be **overpowered** by sparse ones, causing **misclassification**.\n",
    "\n",
    "3. **Non-Spherical Shapes**:\n",
    "   - K-Means performs poorly on **elongated or irregular-shaped clusters**, as it partitions space with **linear boundaries**.\n",
    "\n",
    "## Summary:\n",
    "K-Means struggles when data has clusters with **different sizes, shapes, or densities**, because it assumes uniformity across all clusters.\n",
    "\n",
    "---\n",
    "17. What are the core parameters in DBSCAN, and how do they influence clustering?\n",
    "- \n",
    "In **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**, the two **core parameters** are:\n",
    "\n",
    "### 1. Œµ (epsilon)\n",
    "- Defines the **radius** of a neighborhood around a point.\n",
    "- Within this radius, the algorithm counts how many other points exist.\n",
    "- **Influence**:  \n",
    "  - A **small Œµ** results in many small clusters and a lot of noise.\n",
    "  - A **large Œµ** can merge distinct clusters into one large cluster.\n",
    "\n",
    "### 2. minPts (minimum points)\n",
    "- Specifies the **minimum number of points** required within a point‚Äôs Œµ-radius to consider it a **core point**.\n",
    "- **Influence**:  \n",
    "  - A **high minPts** value requires denser regions to form clusters, making clusters tighter and more selective.\n",
    "  - A **low minPts** value allows looser, more spread-out clusters.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Parameter Influence\n",
    "| Parameter | Small Value | Large Value |\n",
    "|:----------|:------------|:------------|\n",
    "| **Œµ** | Many small clusters, more noise | Few large clusters, less noise |\n",
    "| **minPts** | Loose clusters, easier to form | Dense clusters, harder to form |\n",
    "\n",
    "---\n",
    "18. How does K-Means++ improve upon standard K-Means initialization?\n",
    "-\n",
    "In **standard K-Means**, the initial cluster centers (centroids) are chosen **randomly**.  \n",
    "- **Problem**: Poor initial choices can lead to **bad clustering** (getting stuck in a local minimum) or **slow convergence**.\n",
    "\n",
    "**K-Means++** improves this by:\n",
    "- Choosing the initial centroids **more carefully** to be **spread out**.\n",
    "- This increases the chances of better clustering results.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps of K-Means++ Initialization\n",
    "1. Randomly pick the **first centroid** from the data points.\n",
    "2. For each remaining point, compute its **distance** to the nearest centroid already chosen.\n",
    "3. Choose the next centroid **probabilistically**:\n",
    "   - Points that are **farther away** from the existing centroids are **more likely** to be selected.\n",
    "4. Repeat steps 2‚Äì3 until **k** centroids are chosen.\n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of K-Means++\n",
    "- **Faster convergence** (fewer iterations needed).\n",
    "- **Better clustering quality** (lower chance of poor local minima).\n",
    "- **More consistent results** (less sensitivity to random initialization).\n",
    "\n",
    "---\n",
    "19. What is agglomerative clustering?\n",
    "\n",
    "**Agglomerative Clustering** is a type of **hierarchical clustering** that follows a **bottom-up** approach:\n",
    "- Each data point starts in its **own individual cluster**.\n",
    "- Then, clusters are **repeatedly merged** together based on their **similarity** (or closeness).\n",
    "- This process continues until all points are merged into a **single cluster**, or until a **stopping condition** is met.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Steps in Agglomerative Clustering\n",
    "1. Start with each point as a **separate cluster**.\n",
    "2. Find the **two closest clusters** based on a distance metric (like Euclidean distance).\n",
    "3. **Merge** these two clusters into one.\n",
    "4. **Repeat** steps 2‚Äì3 until:\n",
    "   - All points are merged into one big cluster, or\n",
    "   - A pre-set number of clusters is reached.\n",
    "\n",
    "---\n",
    "\n",
    "### Linkage Criteria (How \"closeness\" is measured)\n",
    "- **Single Linkage**: Distance between the closest points of two clusters.\n",
    "- **Complete Linkage**: Distance between the farthest points of two clusters.\n",
    "- **Average Linkage**: Average distance between all pairs of points in two clusters.\n",
    "- **Ward‚Äôs Linkage**: Merges clusters that minimize the increase in total within-cluster variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Characteristics\n",
    "- Builds a **hierarchical tree** (called a **dendrogram**) showing how clusters are merged.\n",
    "- Does **not** require specifying the number of clusters beforehand (but you can choose how many by cutting the dendrogram).\n",
    "- **Sensitive** to noise and outliers.\n",
    "\n",
    "---\n",
    "20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
    "\n",
    "When evaluating clustering models, two popular metrics are:\n",
    "- **Inertia**: Measures how tightly the points are clustered around the centroids.\n",
    "- **Silhouette Score**: Measures **how well** each point fits within its cluster **compared to other clusters**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Silhouette Score is Better:\n",
    "| Metric | Description | Limitations | Strengths |\n",
    "|:------|:------------|:------------|:----------|\n",
    "| **Inertia** | Sum of squared distances of samples to their nearest cluster center. | Always **decreases** as the number of clusters increases (even if clustering is bad). Hard to compare models directly. | Simple and fast. |\n",
    "| **Silhouette Score** | Combines **cohesion** (how close points are to their cluster) and **separation** (how far points are from other clusters). Values range from **-1 to +1**. | Slightly slower to compute (needs distance calculations). | Gives an **intuitive, normalized** measure of clustering quality. Helps pick the best number of clusters. |\n",
    "\n",
    "---\n",
    "\n",
    "### In Short:\n",
    "- **Inertia** only looks **inside clusters** (compactness).\n",
    "- **Silhouette Score** looks both **inside** and **between clusters** (compactness + separation).\n",
    "- A **higher Silhouette Score** (closer to 1) means **better, more natural clustering**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269508ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a\n",
    "scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa466bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10\n",
    "predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each\n",
    "cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster\n",
    "centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with\n",
    "DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "31.  Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with\n",
    "decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f58e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b225b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot\n",
    "the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ec4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a\n",
    "line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with\n",
    "single linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding\n",
    "noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfbf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the\n",
    "data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64874d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D\n",
    "scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ebf4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the\n",
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b36e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the\n",
    "clustering result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbee5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "42.  Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering.\n",
    "Visualize in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN\n",
    "side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39229c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da9637",
   "metadata": {},
   "outputs": [],
   "source": [
    "45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage.\n",
    "Visualize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759489b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4\n",
    "features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55705629",
   "metadata": {},
   "outputs": [],
   "source": [
    "48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the\n",
    "clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
